* Introduction

  The synet repository is a library for developing networks for
  Synaptics vision chips.  It mainly consists of model component
  definitions organized like so:

  - synet.base
    - low-level network components
  - synet.layers
    - Useful high-level layers
  - synet.katana
    - low-level and high-level layers that run on the Katana chip
  - synet.saber
    - low-level and high-level layers that run on the Saber chip

  The main benefits of using these are as follows:

  - Models built from components in synet.[chip] are guaranteed to run
    on [chip], provided memory constraints are kept (see [[Profiling]]).
  - When (building, exporting, profiling, not sure yet), synet can
    produce profiling information and flag some inefficiencies.
  - Models built from synet and quantized via synet will have only the
    desired components, and will utilize the same padding heuristics.

  In addition to model components, synet also offers a thin wrapper
  around YOLOv5 training which allows you to run YOLOv5 with models defined
  exclusively with synet layers.  Any top-level YOLOv5 function
  (train, detect, val, etc.) will run through synet.  The basic syntax
  is:

  synet [YOLOv5 MODULE] [YOLOv5 ARGS]...

  However, you should use synet's quantize instead of YOLOv5's export
  to convert to a int8 tflite model for deployment (see [[Quantize]]).  We
  give some examples/explanations for basic YOLOv5 usage here, but for
  any further questions about YOLOv5, you should consult the YOLOv5
  github page: [[https://github.com/ultralytics/yolov5]]

* Installation

  In more complex setups, you should create a virtual environment:
  https://docs.python.org/3/library/venv.html

  To install via pip:

  pip install git+ssh://gitms@git.synaptics.com/git/ml/projects/synet.git

  or if you have cloned to a local repository:

  pip install [-e] /PATH/TO/LOCAL/SYNET

  where '-e' will allow you to make edits to your local clone after
  install.

* Train

  The synet repository provides a thin wrapper around YOLOv5 training
  for simple training situations.  The basic usage is

  synet train [YOLOv5 ARGS]

  For instance, if you have a custom dataset prepared in the YOLO
  format, you can train a qVGA (320x240) model for the katana chip
  with

  synet train --cfg katana-qvga.yaml --data /PATH/TO/CUSTOM_DATASET.YAML

  This will put all output at ./runs/train/exp.  See --name --project
  and --exists-ok in the YOLOv5 docs for changing this.  Similarly, if
  you would like to train a VGA (480x640) model (for instance, for
  data with much smaller objects), you could run

  synet train --cfg katana-vga.yaml --data /PATH/TO/CUSTOM_DATASET.YAML

  This will place the best weights at runs/train/exp/weights/best.pt.
  Because Katana has only been used with grayscale cameras, the first
  layer converts to grayscale (not included in tflite export), so your
  dataset can contain a mix of grayscale and color images which get
  treated like grayscale.  Each model yaml knows what resolution is
  compatible with Katana per memory constraints, and so uses that
  resolution by default for training.  The resolution can be
  overridden, with --imgsz, and all training images will be scaled
  according to this resolution, though quantizing will always yield
  the correct resolution model.

* Quantize

  The synet repository also includes the ability to quantize models

  synet quantize --weights MODEL_PT_SAVE --data REP_DATA

  For instance, running:

  synet quantize --weights ./exp/weights/best.pt --data /PATH/TO/CUSTOM_DATASET.YAML

  will create a tflite at ./exp/weights/best.tflite.  You may also
  specify a model yaml like so:

  synet quantize --cfg kqvga3-5.2.yaml --data /PATH/TO/CUSTOM_DATASET.YAML

  This will place a quantized model at ./model.tflite.  This will let
  you inspect the architecture, though it will not be a trained model,
  so the model output will be useless.

* Visualize

  You can visualize your trained model on some data using YOLOv5's
  detect function like so:

  synet detect --weights ./exp/weights/best.pt --source /PATH/TO/IMAGE/VIDEO/URL/ETC

  This will place a modified copy of the --source in
  runs/detect/exp[#] with bounding box annotations added from the
  model you specified.

* Advanced

** Creating Custom Model Architectures

*** Profiling
   
** Custom Training

*** YOLOv5 with Data Subset

    The first step to do to train on a custom dataset is to get the
    data in the YOLO format.  See:
    [[https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data]].
    Generally, datasets have multiple classes.  However, tiny neural
    network models need to be much more specialized, so you generally
    train on only a few classes.  To this end, synet provides one
    additional convenience function to create a data subset with the
    desired classes.  However, it only supports datasets where the
    splits are specified as a directory, not as a text file or list
    (the most common, simple use case).  Suppose you have a dataset
    specified at OLD_YAML with the following content:

    path: /data
    train: images/train
    val: images/val
    names:
      0: bicycle
      1: car
      2: bus

    If you create a new yaml at NEW_YAML with the following content:

    path: /data
    train: images/train_subset
    val: images/val_subset
    names:
      0: bus

    then you can run

    python -m synet data_subset [--max-bg-ratio MAX_BG_RATIO] OLD_YAML NEW_YAML

    Then you can specify --data NEW_YAML for future trainings.  To
    explain, this operation will create new directories at
    /data/images/train_subset, /data/images/val_subset,
    /data/labels/train_subset, and /data/labels/val_subset.  The new
    images directories will be filled with symlinks to images from the
    original, corresponding, directories, and the new labels
    directories will be filled with modified labels with pruned
    classes missing (e.g. car) and kept classes reassigned (2 -> 1).
    Additionally, if --max-bg-ratio is specified, then no more than
    MAX_BG_RATIO of the output dataset will be background samples
    (background samples pruned randomly).  If every sample should have
    at least one label, then set --max-bg-ratio to 0 (not
    recommended).

*** BYO Training Code

    create custom_patches.py in synet from yolov5_patches.py.  In
    train.py add:

    from synet.custom_patches import patch_custom
    patch_custom('katana')

    When specifying a model config, you can either point to a yaml
    copied from this repository (see synet_pip/synet/zoo/*.yaml), or
    you can change your model build call from

    Model(self.cfg or ..., ...)

    to

    Model(synet.zoo.find_model_path(self.cfg) or ..., ...)

    In this second case, you will be able to specify a --cfg with a
    yaml name like 'katana_kvga.yaml', and the yaml from the synet
    repo will be used (backwards compatible, so is a safe change).

** Quantizing from Python

** Modifying and/or Contributing

*** running the test suite

* TODO

  - Finish empty README entries above
  - Decide if base.Grayscale grayscale method should be improved
    - possibly change default Grayscale behavior
  - Asses if depthwise convolutions can be useful with modern
    architectures (now that features are higher earlier on.)
    - possibly add flag for depthwise convs in Inverted Residual
