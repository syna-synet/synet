* Introduction

  The SyNet repository is a library for developing networks for
  Synaptics vision (and eventually maybe other) chips.  It mainly
  consists of model component definitions organized like so:

  - synet.base
    - low-level network components
  - synet.layers
    - Useful high-level layers
  - synet.katana
    - low-level and high-level layers that run on the Katana chip
  - synet.saber
    - low-level and high-level layers that run on the Saber chip

  The main benefits of using these are as follows:

  - Models built from components in synet.[chip] are guaranteed to run
    on [chip], provided memory constraints are kept (see [[Profiling]]).
  - When (building, exporting, profiling, not sure yet), SyNet can
    produce profiling information and flag some inefficiencies.
  - Models built from SyNet and quantized via SyNet will have only the
    desired components, and will utilize the same padding heuristics
    in inference as in training.

  In addition to model components, SyNet also offers a thin wrapper
  around YOLOv5 training which allows you to run YOLOv5 with models defined
  exclusively with SyNet layers.  Any top-level YOLOv5 function
  (train, detect, val, etc.) will run through SyNet.  The basic syntax
  is:

  synet [YOLOv5 MODULE] [YOLOv5 ARGS]...

  However, you should use SyNet's quantize instead of YOLOv5's export
  to convert to a int8 tflite model for deployment (see [[Quantize]]).  We
  give some examples/explanations for basic YOLOv5 usage here, but for
  any further questions about YOLOv5, you should consult the YOLOv5
  github page: [[https://github.com/ultralytics/yolov5]]

* Installation

  In more complex setups, you should create a virtual environment:
  https://docs.python.org/3/library/venv.html

  To install via pip:

  pip install git+ssh://gitms@git.synaptics.com/git/ml/projects/synet.git

  or if you have cloned to a local repository:

  pip install [-e] /PATH/TO/LOCAL/SYNET

  where '-e' will allow you to make edits to your local clone after
  install.

* Shell API

  The basic syntax for running SyNet from a shell is:

  synet [entrypoint] [entrypoint specific args]

  Where entrypoint can be a native SyNet module, or a backend like
  ultralytics.  For instance:

  synet ultralytics train ...
  synet quantize --backend ultralytics ...

  Notice that while some backends are callable this way, the backend
  may also need to be specified for other modules.  For instance,
  synet.quantize needs to know with which backend to the model.
  
** Train

   The SyNet repository provides a thin wrapper around Ultralytics
   training for simple training situations.  The basic usage is

   synet ultralytics [OTHER ULTRALYTICS ARGS]

   For instance, if you want to train a person keypoint model, you can
   train a qVGA (320x240) model for the sabre chip with

   synet ultralytics train model=sabre-keypoint-qvga.yaml data=coco-pose.yaml

   This will put all output at ./runs/train/exp.  See name project and
   exists-ok in the Ultralytics docs for changing this.  The above
   command also tries to download the coco dataset to ./datasets.  The
   best way I have found to deal with this is with a symlink to my
   desired location.

   ln -s /mnt/ml_data/datasets/ultralytics_autodownload ./datasets

   This makes a symlink at ./datasets which points to my datasets
   directory.  Similarly, if you would like to train a VGA (480x640)
   model (for instance, for data with much smaller objects), you could
   run:

   synet ultralytics train model=sabre-keypoint-vga.yaml data=coco-pose.yaml

   This will place the best weights at runs/train/pose/weights/best.pt.
   Each model yaml knows what resolution is compatible with Sabre per
   memory constraints, and so uses that resolution by default for
   training.  The resolution can be overridden, with imgsz=..., and
   all training images will be scaled according to this resolution,
   though quantizing will still default to the compatible resolution.
   "model=" is actually modified if the file does not exist to point
   to a model in the SyNet model zoo.

   Support for YOLOv5 as a backend is currently out of date, but
   support may be added back in the future.

** Quantize

   The SyNet repository also includes the ability to quantize models

   synet quantize --backend BACKEND --weights MODEL_PT_SAVE --data REP_DATA

   For instance, running:

   synet quantize --backend ultralytics --weights ./exp/weights/best.pt --data /PATH/TO/CUSTOM_DATASET.YAML

   will create a tflite at ./exp/weights/best.tflite.  You may also
   specify a model yaml like so:

   synet quantize --backend ultralytics --cfg sabre-keypoint-qvga.yaml

   This will place a quantized model at ./model.tflite.  This will let
   you inspect the architecture, though it will not be a trained model,
   so the model output will be useless.

** Visualize

* Python API

  SyNet exists to be the glue between State of the Art training, and
  our chips.  Each model component knows how to "export itself" to a
  keras/tensorflow model.  This done approximately like so:

  from keras import Input, Model
  from synet.base import askeras
  inp = Input(...)
  with askeras:
      kmodel = Model(inp, model(inp))

  (For a more complex example, see quantize.py.)

  So long as only SyNet components actually operate on the model
  input, this method will work.  SyNet integrates with other libraries
  as much as possible, but can be used stand-alone in other python
  project as a library itself.

** Creating Custom Model Architectures

*** Profiling
   
** Custom Training

*** YOLOv5 with Data Subset

    The first step to do to train on a custom dataset is to get the
    data in the YOLO format.  See:
    [[https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data]].
    Generally, datasets have multiple classes.  However, tiny neural
    network models need to be much more specialized, so you generally
    train on only a few classes.  To this end, SyNet provides one
    additional convenience function to create a data subset with the
    desired classes.  However, it only supports datasets where the
    splits are specified as a directory, not as a text file or list
    (the most common, simple use case).  Suppose you have a dataset
    specified at OLD_YAML with the following content:

    path: /data
    train: images/train
    val: images/val
    names:
      0: bicycle
      1: car
      2: bus

    If you create a new yaml at NEW_YAML with the following content:

    path: /data
    train: images/train_subset
    val: images/val_subset
    names:
      0: bus

    then you can run

    python -m synet data_subset [--max-bg-ratio MAX_BG_RATIO] OLD_YAML NEW_YAML

    Then you can specify --data NEW_YAML for future trainings.  To
    explain, this operation will create new directories at
    /data/images/train_subset, /data/images/val_subset,
    /data/labels/train_subset, and /data/labels/val_subset.  The new
    images directories will be filled with symlinks to images from the
    original, corresponding, directories, and the new labels
    directories will be filled with modified labels with pruned
    classes missing (e.g. car) and kept classes reassigned (2 -> 1).
    Additionally, if --max-bg-ratio is specified, then no more than
    MAX_BG_RATIO of the output dataset will be background samples
    (background samples pruned randomly).  If every sample should have
    at least one label, then set --max-bg-ratio to 0 (not
    recommended).

*** From modified YOLOv5 code

    If your training code is a fork of yolo, these steps may be more
    appropriate.  Create custom_patches.py in SyNet from
    yolov5_patches.py.  In train.py add:

    from synet.custom_patches import patch_custom
    patch_custom('katana')

    When specifying a model config, you can either point to a yaml
    copied from this repository (see synet_pip/synet/zoo/*.yaml), or
    you can change your model build call from

    Model(self.cfg or ..., ...)

    to

    Model(synet.zoo.find_model_path(self.cfg) or ..., ...)

    In this second case, you will be able to specify a --cfg with a
    yaml name like 'katana-kvga.yaml', and the yaml from the SyNet
    repo will be used (backwards compatible, so is a safe change).

** Quantizing from Python

*** Converting to Keras

    After you load your model (like in [[BYO Pytorch Training Code]]), you
    can convert your model to keras by using the as_keras context
    manager.  For example, to quantize a 240x320, batch_size=1 model:

    from synet import as_keras, get_model
    from keras import Model, Input
    torch_model = get_model("/path/to/model.pt")
    inp = Input((240, 320, 1), batch_size=1)
    with as_keras(imgsz=(240, 320)):
        keras_model = Model(inp, torch_model(inp))

*** Quantizing to tflite

    Once you have obtained as keras model as shown in [[Converting to
    Keras]], you can obtain a quantized model using the test (falling
    back to val) split of a dataset in the YOLOv5 format like os:

    from synet.quantize import quantize
    quantize(keras_models, "/path/to/data.yaml", (320,240),
             number=500, out_path="/desired/output/path.tflite")

    This will quantize a keras model using 500 samples from the data
    yaml, saving the output to /desired/output/path.tflite.  If
    out_path is omitted, the tflite buffer is returned instead.

** Modifying and/or Contributing

*** running the test suite

* Contributing

** Coding Style
   While you should generally conform to pep8 and the like, an exception is made for

** TODO

   - Need chip-specific export
     - functionality should go in synet/<chip>.py or
       synet/<chip>/quantize.py or synet/<chip>/__init__.py
   - Need better backend organization.  Maybe in
     synet/backends/<backend>.py
   - Finish empty README entries above
   - Decide if base.Grayscale grayscale method should be improved
     - possibly change default Grayscale behavior
