* Introduction

  The SyNet repository is a library for developing networks for
  Synaptics vision chips.  It consists of PyTorch model components
  which can be exported to tensorflow and tflite without an
  intermediate export backend like ONNX.  The resulting tflite files
  are clean, and respect chip memory constraints.  The aim of SyNet is
  to streamline the process of generating trained models to deploy on
  Synaptics chips, for internal and external use.

  In addition to model definitions, analysis, data manipulation, and
  data analysis tools, SyNet also aims to support several "backends".
  For now, the only backend supported in Ultralytics.  Ultralytics
  provides a suit of tools for training vision models and visualizing
  those models.  Using Ultralytics as a backend, you can generate
  trained vision models optimized for our chips.

* Installation

  In more complex setups, you should create a virtual environment:
  https://docs.python.org/3/library/venv.html

  In the following examples, we include the ultralytics backend.  For
  a core SyNet install, omit the '[ultra]' below.

  To install via pip:

  pip install git+ssh://git@gitlab.synaptics.com:wssd-ai-algorithms/synet-fork.git[ultra]

  or if you have cloned to a local repository:

  pip install [-e] /PATH/TO/LOCAL/SYNET[ultra]

  where '-e' will allow you to make edits to your local clone after
  install.

* Roadmap

** Current Features

   - Models optimized for Sabre
   - Memory and compute efficient model components
   - Useable with PyTorch and Tensorflow libraries
   - Useable from the command line or python environment
   - In-model demosaicing export option
     - fater than demosaic hardware block
     - increases available weight memory by >2x
   - Includes slim tflite runtime utilities
   - Has the ability to support training backends
     - Main backend is Ultralytics
     - Currently supports 3/4 visions tasks from Ultralytics
     - Supports evaluating tflites through Ultralytics
     - Allows for easy setup of laptop demos
       - quickly run and view model running on webcam
   - Includes more advanced custom tflite evaluation (not through
     Ultralytics)

** Planned 1.0 Features

   - Models optimized for VS680
   - Autodownload of pretrained models
   - Support the last Ultralytics task (Classification).

** Planned for later releases

   - Automatic model selection from zoo
     - Select training resolution, inference resolution, and heads.

** Miscelaneous Tasks

   - Investigate the demosaic descrepancy.
   - clean up metrics.py.
   - remove unused files.
   - test that tflite output labeling is consistent between different
     head numbers.
   - Revise comments and docstrings.
   - Need chip-specific export
     - functionality should go in synet/<chip>.py
   - Decide if base.Grayscale grayscale method should be improved
     - possibly change default Grayscale behavior
     - support variable channel count in backend

* Core Shell API

  The basic syntax for running SyNet from a shell is:

  #+begin_src shell
    synet [entrypoint] [entrypoint specific args]
  #+end_src

  Where entrypoint can be a native SyNet module, or a backend like
  ultralytics.  For instance:

  #+begin_src shell
    synet ultralytics train ...
    synet quantize --backend ultralytics ...
  #+end_src

  Notice that while some backends are callable this way, the backend
  may also need to be specified for other modules.  For instance,
  synet.quantize needs to know with which backend to load the model.
  
** Quantize

   The SyNet repository also includes the ability to quantize models

   synet quantize --backend BACKEND --weights MODEL_PT_SAVE --data REP_DATA

   For instance, running:

   synet quantize --backend ultralytics --weights ./exp/weights/best.pt --data /PATH/TO/CUSTOM_DATASET.YAML

   will create a tflite at ./exp/weights/best.tflite.  You may also
   specify a model yaml like so:

   synet quantize --backend ultralytics --cfg sabre-keypoint-qvga.yaml

   This will place a quantized model at ./model.tflite.  This will
   let you inspect the architecture, though it will not be a trained
   model, so the model output will be useless.

* Core Python API

  SyNet exists to be the glue between State of the Art training, and
  our chips.  Each model component knows how to "export itself" to a
  keras/tensorflow model.  This done approximately like so:

  from keras import Input, Model
  from synet.base import askeras
  inp = Input(...)
  with askeras:
      kmodel = Model(inp, model(inp))

  (For a more complex example, see quantize.py.)

  So long as only SyNet components actually operate on the model
  input, this method will work.  SyNet integrates with other libraries
  as much as possible, but can be used stand-alone in other python
  project as a library itself.

** Ultralytics

   from synet.backends.ultralytics import Backend
   Backend().patch()

** Creating Custom Model Architectures

*** Profiling
   
** Custom Training

*** YOLOv5 with Data Subset

    The first step to do to train on a custom dataset is to get the
    data in the YOLO format.  See:
    [[https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data]].
    Generally, datasets have multiple classes.  However, tiny neural
    network models need to be much more specialized, so you generally
    train on only a few classes.  To this end, SyNet provides one
    additional convenience function to create a data subset with the
    desired classes.  However, it only supports datasets where the
    splits are specified as a directory, not as a text file or list
    (the most common, simple use case).  Suppose you have a dataset
    specified at OLD_YAML with the following content:

    path: /data
    train: images/train
    val: images/val
    names:
      0: bicycle
      1: car
      2: bus

    If you create a new yaml at NEW_YAML with the following content:

    path: /data
    train: images/train_subset
    val: images/val_subset
    names:
      0: bus

    then you can run

    python -m synet data_subset [--max-bg-ratio MAX_BG_RATIO] OLD_YAML NEW_YAML

    Then you can specify --data NEW_YAML for future trainings.  To
    explain, this operation will create new directories at
    /data/images/train_subset, /data/images/val_subset,
    /data/labels/train_subset, and /data/labels/val_subset.  The new
    images directories will be filled with symlinks to images from the
    original, corresponding, directories, and the new labels
    directories will be filled with modified labels with pruned
    classes missing (e.g. car) and kept classes reassigned (2 -> 1).
    Additionally, if --max-bg-ratio is specified, then no more than
    MAX_BG_RATIO of the output dataset will be background samples
    (background samples pruned randomly).  If every sample should have
    at least one label, then set --max-bg-ratio to 0 (not
    recommended).

*** From modified YOLOv5 code

    If your training code is a fork of yolo, these steps may be more
    appropriate.  Create custom_patches.py in SyNet from
    yolov5_patches.py.  In train.py add:

    from synet.custom_patches import patch_custom
    patch_custom('katana')

    When specifying a model config, you can either point to a yaml
    copied from this repository (see synet_pip/synet/zoo/*.yaml), or
    you can change your model build call from

    #+begin_src python
      Model(self.cfg or ..., ...)
    #+end_src

    to

    #+begin_src python
      Model(synet.zoo.find_model_path(self.cfg) or ..., ...)
    #+end_src

    In this second case, you will be able to specify a --cfg with a
    yaml name like 'katana-kvga.yaml', and the yaml from the SyNet
    repo will be used (backwards compatible, so is a safe change).

** Quantizing from Python

*** Converting to Keras

    After you load your model (like in BYO Pytorch Training Code), you
    can convert your model to keras by using the as_keras context
    manager.  For example, to quantize a 240x320, batch_size=1 model:

    #+begin_src python
      from synet import as_keras, get_model
      from keras import Model, Input
      torch_model = get_model("/path/to/model.pt")
      inp = Input((240, 320, 1), batch_size=1)
      with as_keras(imgsz=(240, 320)):
          keras_model = Model(inp, torch_model(inp))
    #+end_src

*** Quantizing to tflite

    Once you have obtained as keras model as shown in [[Converting to
    Keras]], you can obtain a quantized model using the test (falling
    back to val) split of a dataset in the YOLOv5 format like os:

    from synet.quantize import quantize
    quantize(keras_models, "/path/to/data.yaml", (320,240),
             number=500, out_path="/desired/output/path.tflite")

    This will quantize a keras model using 500 samples from the data
    yaml, saving the output to /desired/output/path.tflite.  If
    out_path is omitted, the tflite buffer is returned instead.

* Backends

  For now, the only backend supported is Ultralytics.

** Ultralytics

   Any Ultralytics function (train, predict, val, etc.) will run
   through SyNet with SyNet modules.  The basic syntax is:

   synet ultralytics [ultralytics ARGS]...

   However, you should use SyNet's quantize instead of Ultralytics'
   export to convert to a quantized tflite model for deployment.  

   We give some examples/explanations for basic
   Ultralytics usage here, but for any further questions about
   Ultralytics, you should consult the Ultralytics github page:
   [[https://github.com/ultralytics/ultralytics]]

*** Train

    The SyNet repository provides a thin wrapper around Ultralytics
    training for simple training situations.  The basic usage is

    synet ultralytics [OTHER ULTRALYTICS ARGS]

    For instance, if you want to train a person keypoint model, you
    can train a qVGA (320x240) model for the sabre chip with

    synet ultralytics train model=sabre-keypoint-qvga.yaml data=coco-pose.yaml

    This will put all output at ./runs/train/exp.  See name project
    and exists-ok in the Ultralytics docs for changing this.  The
    above command also tries to download the coco dataset to
    ./datasets.  The best way I have found to deal with this is with a
    symlink to my desired location.

    ln -s /mnt/ml_data/datasets/ultralytics_autodownload ./datasets

    This makes a symlink at ./datasets which points to my datasets
    directory.  Similarly, if you would like to train a VGA (480x640)
    model (for instance, for data with much smaller objects), you
    could run:

    synet ultralytics train model=sabre-keypoint-vga.yaml data=coco-pose.yaml

    This will place the best weights at
    runs/train/pose/weights/best.pt.  Each model yaml knows what
    resolution is compatible with Sabre per memory constraints, and so
    uses that resolution by default for training.  The resolution can
    be overridden, with imgsz=..., and all training images will be
    scaled according to this resolution, though quantizing will still
    default to the compatible resolution.  "model=" is actually
    modified, if the file does not exist, to point to a model in the
    SyNet model zoo.

* Contributing

** Test Suite

   Please run the test suite before pushing ANY changes upstream with:

   #+begin_src shell
     pytest -v
   #+end_src

** Docstring Style

   Docstrings conform to numpy, scipy, and scikits docstring conventions:
   https://numpydoc.readthedocs.io/en/latest/format.html

** Imports

   Only quantize.py and tflite_utils.py should import tensorflow at
   the top of the file.  Otherwise, tensorflow modules should be
   impored at the beginning of functions where they are used.  This
   ensures tensorflow is only loaded when strictly necessary.

   Only backends/ultralytics.py should directly import anything from
   ultralytics, and backends.ultralytics should only be accessed by
   obtaining the ultralytics backend from backends.get_backend().
